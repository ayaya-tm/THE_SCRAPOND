import json
from janome.tokenizer import Tokenizer
from collections import Counter

# ãƒ„ã‚¤ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
with open("./tweets.json", "r", encoding="utf-8") as f:
    tweets = json.load(f)

# å½¢æ…‹ç´ è§£æå™¨ã®æº–å‚™
tokenizer = Tokenizer()

words = []
stopwords = set([
    "ã‚¶ã‚»ã‚«ãƒ³ãƒ‰", "the", "second", "ã‚ã‚‹", "ã•ã‚“", "ã™ã‚‹", "ã‚’", "ã«", "ã§", "ã¨", "ã‚‚",
    "ã§ã™", "ã¾ã™", "ã ", "THE", "SECOND", "://", "com", "co", "//", "https"
])

for tweet in tweets:
    tokens = tokenizer.tokenize(tweet)
    for token in tokens:
        pos = token.part_of_speech.split(',')[0]
        if pos == "åè©":
            word = token.surface
        elif pos in ["å‹•è©", "å½¢å®¹è©"]:
            word = token.base_form
        else:
            word = None

        if word:
            if (
                len(word) >= 2 and
                word not in stopwords and
                not word.isdigit()  # âœ… æ•°å­—ã®ã¿ã¯é™¤å¤–
            ):
                words.append(word)

# é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°
counter = Counter(words)
ranking = counter.most_common(50)  # ä¸Šä½50å˜èªã‚’å–å¾—

# çµæœã‚’ä¿å­˜
with open("./lib/word_ranking.json", "w", encoding="utf-8") as f:
    json.dump(ranking, f, ensure_ascii=False, indent=2)

print("ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ word_ranking.json ã«ä¿å­˜ã—ã¾ã—ãŸï¼")
